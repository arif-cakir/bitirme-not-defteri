{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [KDNUGGETS](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)<br>\n",
    "### TF-IDF — Term Frequency-Inverse Document Frequency: \n",
    "The text can be in the form of a document or various documents (corpus). It is a combination of two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF).<br><br>\n",
    "TF counts number of the words in the document. Where i is occurance of a word, j is the words in the document TF can be shown as following:\n",
    "$$TF(i) =   \\frac{\\log(Frequency(i,j))}{\\log(TotalNumber(j))}$$\n",
    "<br><br>\n",
    "IDF is rarity of the words. Words that are rarely used in the corpus may hold significant information. Where d is documents and i is occurance of a word, IDF can be showed as following:\n",
    "\n",
    "$$IDF (i) = \\log (\\frac{TotalNumber(d)}{Frequency(d,i)})$$\n",
    "\n",
    "### Word2Vec:\n",
    "Word2Vec uses cosine similarity metric. If the cosine angle is 0 and cosine value is 1, that means words are overlapping. If the cosine angle is 90 and cosine value is 0, that means words are independent or hold no contextual similarity. Word2Vec offers two neural network-based variants: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "\n",
    "In CBOW, the neural network model takes various words as input and predicts the target word that is closely related to the context of the input words. On the other hand, the Skip-gram architecture takes one word as input and predicts its closely related context words.\n",
    "\n",
    "Skip-gram <br>\n",
    "\n",
    "$$ P (w_o | w_c) = \\frac{e^{u_o^\\intercal v_c}}{\\sum e^{u_i^\\intercal v_c}}$$\n",
    "\n",
    "<br>\n",
    "Word2Vec only captures the local context of words \n",
    "\n",
    "![CBOW vs Skip-gram](https://www.kdnuggets.com/wp-content/uploads/nlp-cbow-skip-gram.jpg)\n",
    "\n",
    "Although one-hot word vectors are easy to construct, they are usually not a good choice. A main reason is that one-hot word vectors cannot accurately express the similarity between different words, such as the cosine similarity that we often use. \n",
    "\n",
    "\n",
    "### GloVe — Global Vectors for Word Representation\n",
    "GloVe considers the entire corpus and creates a large matrix that can capture the co-occurrence of words within the corpus. GloVe combines two-word vector learning methods: matrix factorization and local context window method. GloVe technique has a simpler least square cost or error function that reduces the computational cost of training the model. The resulting word embeddings are different and improved.\n",
    "\n",
    "![GloVe](https://www.kdnuggets.com/wp-content/uploads/nlp-glove-embedding-example.jpg)\n",
    "\n",
    "\n",
    "### BERT — Bidirectional Encoder Representations from Transformers\n",
    "BERT-Base has 110 million parameters, and BERT-Large has 340 million parameters. During the training process, embeddings are refined by passing through each BERT encoder layer. For each word, the attention mechanism captures word associations based on the words on the left and the words on the right. Word embeddings are also positionally encoded to keep track of the pattern or position of each word in a sentence. Google search engine uses BERT.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchial Softmax\n",
    "\n",
    "![image.png](https://d2l.ai/_images/hi-softmax.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c28da407b5413b3940d87ecdae5ea8ce0c2929d84f560e9f5daaaa2573d53e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
